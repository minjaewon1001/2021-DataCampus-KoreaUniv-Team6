{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d119f3-911b-4587-b726-1684ab142ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "from multiprocessing import Pool\n",
    "import argparse\n",
    "from collect_links import CollectLinks\n",
    "import imghdr\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e520883-63c3-4a4b-aa65-1e80c2a55c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--skip SKIP] [--threads THREADS]\n",
      "                             [--instagram INSTAGRAM] [--full FULL]\n",
      "                             [--face FACE] [--no_gui NO_GUI] [--limit LIMIT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\zorang2\\AppData\\Roaming\\jupyter\\runtime\\kernel-5e26278d-acdb-486c-8cd7-ea9d37e54f1e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zorang2\\anaconda3\\envs\\py37_tf24\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3449: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "class Sites:\n",
    "    INSTAGRAM = 1\n",
    "    #NAVER = 2\n",
    "    GOOGLE_FULL = 3\n",
    "    #NAVER_FULL = 4\n",
    "\n",
    "    @staticmethod\n",
    "    def get_text(code):\n",
    "        if code == Sites.INTAGRAM:\n",
    "            return 'instagram'\n",
    "        #elif code == Sites.NAVER:\n",
    "        #    return 'naver'\n",
    "        elif code == Sites.INSTAGRAM_FULL:\n",
    "            return 'instagram'\n",
    "        #elif code == Sites.NAVER_FULL:\n",
    "        #    return 'naver'\n",
    "\n",
    "    @staticmethod\n",
    "    def get_face_url(code):\n",
    "        if code == Sites.INSTAGRAM or Sites.INSTAGRAM_FULL:\n",
    "            return \"&tbs=itp:face\"\n",
    "        #if code == Sites.NAVER or Sites.NAVER_FULL:\n",
    "        #    return \"&face=1\"\n",
    "\n",
    "\n",
    "class AutoCrawler:\n",
    "    def __init__(self, skip_already_exist=True, n_threads=4, do_instagram=True, download_path='download',\n",
    "                 full_resolution=False, face=False, no_gui=False, limit=0):\n",
    "        \"\"\"\n",
    "        :param skip_already_exist: Skips keyword already downloaded before. This is needed when re-downloading.\n",
    "        :param n_threads: Number of threads to download.\n",
    "        :param do_google: Download from google.com (boolean)\n",
    "        :param do_naver: Download from naver.com (boolean)\n",
    "        :param download_path: Download folder path\n",
    "        :param full_resolution: Download full resolution image instead of thumbnails (slow)\n",
    "        :param face: Face search mode\n",
    "        :param no_gui: No GUI mode. Acceleration for full_resolution mode.\n",
    "        :param limit: Maximum count of images to download. (0: infinite)\n",
    "        \"\"\"\n",
    "\n",
    "        self.skip = skip_already_exist\n",
    "        self.n_threads = n_threads\n",
    "        self.do_instagram = do_instagram\n",
    "        #self.do_naver = do_naver\n",
    "        self.download_path = download_path\n",
    "        self.full_resolution = full_resolution\n",
    "        self.face = face\n",
    "        self.no_gui = no_gui\n",
    "        self.limit = limit\n",
    "\n",
    "        os.makedirs('./{}'.format(self.download_path), exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def all_dirs(path):\n",
    "        paths = []\n",
    "        for dir in os.listdir(path):\n",
    "            if os.path.isdir(path + '/' + dir):\n",
    "                paths.append(path + '/' + dir)\n",
    "\n",
    "        return paths\n",
    "\n",
    "    @staticmethod\n",
    "    def all_files(path):\n",
    "        paths = []\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                if os.path.isfile(path + '/' + file):\n",
    "                    paths.append(path + '/' + file)\n",
    "\n",
    "        return paths\n",
    "\n",
    "    @staticmethod\n",
    "    def get_extension_from_link(link, default='jpg'):\n",
    "        splits = str(link).split('.')\n",
    "        if len(splits) == 0:\n",
    "            return default\n",
    "        ext = splits[-1].lower()\n",
    "        if ext == 'jpg' or ext == 'jpeg':\n",
    "            return 'jpg'\n",
    "        elif ext == 'gif':\n",
    "            return 'gif'\n",
    "        elif ext == 'png':\n",
    "            return 'png'\n",
    "        else:\n",
    "            return default\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_image(path):\n",
    "        ext = imghdr.what(path)\n",
    "        if ext == 'jpeg':\n",
    "            ext = 'jpg'\n",
    "        return ext  # returns None if not valid\n",
    "\n",
    "    @staticmethod\n",
    "    def make_dir(dirname):\n",
    "        current_path = os.getcwd()\n",
    "        path = os.path.join(current_path, dirname)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_keywords(keywords_file='keywords.txt'):\n",
    "        # read search keywords from file\n",
    "        with open(keywords_file, 'r', encoding='utf-8-sig') as f:\n",
    "            text = f.read()\n",
    "            lines = text.split('\\n')\n",
    "            lines = filter(lambda x: x != '' and x is not None, lines)\n",
    "            keywords = sorted(set(lines))\n",
    "\n",
    "        print('{} keywords found: {}'.format(len(keywords), keywords))\n",
    "\n",
    "        # re-save sorted keywords\n",
    "        with open(keywords_file, 'w+', encoding='utf-8') as f:\n",
    "            for keyword in keywords:\n",
    "                f.write('{}\\n'.format(keyword))\n",
    "\n",
    "        return keywords\n",
    "\n",
    "    @staticmethod\n",
    "    def save_object_to_file(object, file_path, is_base64=False):\n",
    "        try:\n",
    "            with open('{}'.format(file_path), 'wb') as file:\n",
    "                if is_base64:\n",
    "                    file.write(object)\n",
    "                else:\n",
    "                    shutil.copyfileobj(object.raw, file)\n",
    "        except Exception as e:\n",
    "            print('Save failed - {}'.format(e))\n",
    "\n",
    "    @staticmethod\n",
    "    def base64_to_object(src):\n",
    "        header, encoded = str(src).split(',', 1)\n",
    "        data = base64.decodebytes(bytes(encoded, encoding='utf-8'))\n",
    "        return data\n",
    "\n",
    "    def download_images(self, keyword, links, site_name, max_count=0):\n",
    "        self.make_dir('{}/{}'.format(self.download_path, keyword.replace('\"', '')))\n",
    "        total = len(links)\n",
    "        success_count = 0\n",
    "\n",
    "        if max_count == 0:\n",
    "            max_count = total\n",
    "\n",
    "        for index, link in enumerate(links):\n",
    "            if success_count >= max_count:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                print('Downloading {} from {}: {} / {}'.format(keyword, site_name, success_count + 1, max_count))\n",
    "\n",
    "                if str(link).startswith('data:image/jpeg;base64'):\n",
    "                    response = self.base64_to_object(link)\n",
    "                    ext = 'jpg'\n",
    "                    is_base64 = True\n",
    "                elif str(link).startswith('data:image/png;base64'):\n",
    "                    response = self.base64_to_object(link)\n",
    "                    ext = 'png'\n",
    "                    is_base64 = True\n",
    "                else:\n",
    "                    response = requests.get(link, stream=True)\n",
    "                    ext = self.get_extension_from_link(link)\n",
    "                    is_base64 = False\n",
    "\n",
    "                no_ext_path = '{}/{}/{}_{}'.format(self.download_path.replace('\"', ''), keyword, site_name, str(index).zfill(4))\n",
    "                path = no_ext_path + '.' + ext\n",
    "                self.save_object_to_file(response, path, is_base64=is_base64)\n",
    "\n",
    "                success_count += 1\n",
    "                del response\n",
    "\n",
    "                ext2 = self.validate_image(path)\n",
    "                if ext2 is None:\n",
    "                    print('Unreadable file - {}'.format(link))\n",
    "                    os.remove(path)\n",
    "                    success_count -= 1\n",
    "                else:\n",
    "                    if ext != ext2:\n",
    "                        path2 = no_ext_path + '.' + ext2\n",
    "                        os.rename(path, path2)\n",
    "                        print('Renamed extension {} -> {}'.format(ext, ext2))\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Download failed - ', e)\n",
    "                continue\n",
    "\n",
    "    def download_from_site(self, keyword, site_code):\n",
    "        site_name = Sites.get_text(site_code)\n",
    "        add_url = Sites.get_face_url(site_code) if self.face else \"\"\n",
    "\n",
    "        try:\n",
    "            collect = CollectLinks(no_gui=self.no_gui)  # initialize chrome driver\n",
    "        except Exception as e:\n",
    "            print('Error occurred while initializing chromedriver - {}'.format(e))\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            print('Collecting links... {} from {}'.format(keyword, site_name))\n",
    "\n",
    "            if site_code == Sites.INSTAGRAM:\n",
    "                links = collect.instagram(keyword, add_url)\n",
    "\n",
    "            #elif site_code == Sites.NAVER:\n",
    "            #    links = collect.naver(keyword, add_url)\n",
    "\n",
    "            elif site_code == Sites.INSTAGRAM_FULL:\n",
    "                links = collect.instagram_full(keyword, add_url)\n",
    "\n",
    "            #elif site_code == Sites.NAVER_FULL:\n",
    "            #    links = collect.naver_full(keyword, add_url)\n",
    "\n",
    "            else:\n",
    "                print('Invalid Site Code')\n",
    "                links = []\n",
    "\n",
    "            print('Downloading images from collected links... {} from {}'.format(keyword, site_name))\n",
    "            self.download_images(keyword, links, site_name, max_count=self.limit)\n",
    "\n",
    "            print('Done {} : {}'.format(site_name, keyword))\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Exception {}:{} - {}'.format(site_name, keyword, e))\n",
    "\n",
    "    def download(self, args):\n",
    "        self.download_from_site(keyword=args[0], site_code=args[1])\n",
    "\n",
    "    def do_crawling(self):\n",
    "        keywords = self.get_keywords()\n",
    "\n",
    "        tasks = []\n",
    "\n",
    "        for keyword in keywords:\n",
    "            dir_name = '{}/{}'.format(self.download_path, keyword)\n",
    "            if os.path.exists(os.path.join(os.getcwd(), dir_name)) and self.skip:\n",
    "                print('Skipping already existing directory {}'.format(dir_name))\n",
    "                continue\n",
    "\n",
    "            if self.do_instagram:\n",
    "                if self.full_resolution:\n",
    "                    tasks.append([keyword, Sites.instagram_FULL])\n",
    "                else:\n",
    "                    tasks.append([keyword, Sites.INSTAGRAM])\n",
    "\n",
    "            #if self.do_naver:\n",
    "            #    if self.full_resolution:\n",
    "            #        tasks.append([keyword, Sites.NAVER_FULL])\n",
    "            #    else:\n",
    "            #        tasks.append([keyword, Sites.NAVER])\n",
    "\n",
    "        pool = Pool(self.n_threads)\n",
    "        pool.map_async(self.download, tasks)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        print('Task ended. Pool join.')\n",
    "\n",
    "        self.imbalance_check()\n",
    "\n",
    "        print('End Program')\n",
    "\n",
    "    def imbalance_check(self):\n",
    "        print('Data imbalance checking...')\n",
    "\n",
    "        dict_num_files = {}\n",
    "\n",
    "        for dir in self.all_dirs(self.download_path):\n",
    "            n_files = len(self.all_files(dir))\n",
    "            dict_num_files[dir] = n_files\n",
    "\n",
    "        avg = 0\n",
    "        for dir, n_files in dict_num_files.items():\n",
    "            avg += n_files / len(dict_num_files)\n",
    "            print('dir: {}, file_count: {}'.format(dir, n_files))\n",
    "\n",
    "        dict_too_small = {}\n",
    "\n",
    "        for dir, n_files in dict_num_files.items():\n",
    "            if n_files < avg * 0.5:\n",
    "                dict_too_small[dir] = n_files\n",
    "\n",
    "        if len(dict_too_small) >= 1:\n",
    "            print('Data imbalance detected.')\n",
    "            print('Below keywords have smaller than 50% of average file count.')\n",
    "            print('I recommend you to remove these directories and re-download for that keyword.')\n",
    "            print('_________________________________')\n",
    "            print('Too small file count directories:')\n",
    "            for dir, n_files in dict_too_small.items():\n",
    "                print('dir: {}, file_count: {}'.format(dir, n_files))\n",
    "\n",
    "            print(\"Remove directories above? (y/n)\")\n",
    "            answer = input()\n",
    "\n",
    "            if answer == 'y':\n",
    "                # removing directories too small files\n",
    "                print(\"Removing too small file count directories...\")\n",
    "                for dir, n_files in dict_too_small.items():\n",
    "                    shutil.rmtree(dir)\n",
    "                    print('Removed {}'.format(dir))\n",
    "\n",
    "                print('Now re-run this program to re-download removed files. (with skip_already_exist=True)')\n",
    "        else:\n",
    "            print('Data imbalance not detected.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--skip', type=str, default='true',\n",
    "                        help='Skips keyword already downloaded before. This is needed when re-downloading.')\n",
    "    parser.add_argument('--threads', type=int, default=4, help='Number of threads to download.')\n",
    "    parser.add_argument('--instagram', type=str, default='true', help='Download from instagram.com (boolean)')\n",
    "    #parser.add_argument('--naver', type=str, default='true', help='Download from naver.com (boolean)')\n",
    "    parser.add_argument('--full', type=str, default='false', help='Download full resolution image instead of thumbnails (slow)')\n",
    "    parser.add_argument('--face', type=str, default='false', help='Face search mode')\n",
    "    parser.add_argument('--no_gui', type=str, default='auto', help='No GUI mode. Acceleration for full_resolution mode. '\n",
    "                                                                   'But unstable on thumbnail mode. '\n",
    "                                                                    'Default: \"auto\" - false if full=false, true if full=true')\n",
    "    parser.add_argument('--limit', type=int, default=0, help='Maximum count of images to download per site. (0: infinite)')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    _skip = False if str(args.skip).lower() == 'false' else True\n",
    "    _threads = args.threads\n",
    "    _instagram = False if str(args.instagram).lower() == 'false' else True\n",
    "    #_naver = False if str(args.naver).lower() == 'false' else True\n",
    "    _full = False if str(args.full).lower() == 'false' else True\n",
    "    _face = False if str(args.face).lower() == 'false' else True\n",
    "    _limit = int(args.limit)\n",
    "\n",
    "    no_gui_input = str(args.no_gui).lower()\n",
    "    if no_gui_input == 'auto':\n",
    "        _no_gui = _full\n",
    "    elif no_gui_input == 'true':\n",
    "        _no_gui = True\n",
    "    else:\n",
    "        _no_gui = False\n",
    "\n",
    "    print('Options - skip:{}, threads:{}, instagram:{}, full_resolution:{}, face:{}, no_gui:{}, limit:{}'\n",
    "          .format(_skip, _threads, _instagram, _full, _face, _no_gui, _limit))\n",
    "\n",
    "    crawler = AutoCrawler(skip_already_exist=_skip, n_threads=_threads,\n",
    "                          do_instagram=_instagram, full_resolution=_full,\n",
    "                          face=_face, no_gui=_no_gui, limit=_limit)\n",
    "    crawler.do_crawling()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef44ac1-c7da-4a88-86e1-e64bac2556a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[py37_tf24]",
   "language": "python",
   "name": "py37_tf24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
